{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyWK1FDmMX6z",
        "outputId": "50871b54-1b6e-497b-8254-1e904ac02130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\jishn\\anaconda3\\lib\\site-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in c:\\users\\jishn\\anaconda3\\lib\\site-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\jishn\\anaconda3\\lib\\site-packages (1.5.1)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\jishn\\anaconda3\\lib\\site-packages (3.9.2)\n",
            "Requirement already satisfied: seaborn in c:\\users\\jishn\\anaconda3\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\jishn\\anaconda3\\lib\\site-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\jishn\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install numpy pandas scikit-learn matplotlib seaborn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xKbV-TcKMbAs"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def convert_timestamp(ts_str):\n",
        "    try:\n",
        "        h, m, s = ts_str.split(\":\")\n",
        "        return int(h) * 3600 + int(m) * 60 + int(s)\n",
        "    except:\n",
        "        return 3600\n",
        "\n",
        "\n",
        "def load_json_data(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def create_features(data):\n",
        "    records = data.get(\"activityLog\", [])\n",
        "    if not records:\n",
        "        return np.array([]), None\n",
        "\n",
        "    feature_list, descriptions = [], []\n",
        "    for rec in records:\n",
        "        ts_val = convert_timestamp(rec.get(\"timeStampInVideo\", \"0:0:0\"))\n",
        "        count = rec.get(\"count\", 0)\n",
        "        description = rec.get(\"activityDescription\", \"\")\n",
        "        feature_list.append([ts_val, count])\n",
        "        descriptions.append(description)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    desc_encoded = encoder.fit_transform(descriptions)\n",
        "    features = np.hstack([np.array(feature_list), desc_encoded.reshape(-1, 1)])\n",
        "    return features, encoder, descriptions\n",
        "\n",
        "\n",
        "def apply_pca(features, variance_ratio=0.95):\n",
        "    scaler = StandardScaler()\n",
        "    print(features)\n",
        "    scaled_features = scaler.fit_transform(features)\n",
        "    pca = PCA(n_components=variance_ratio)\n",
        "    reduced_features = pca.fit_transform(scaled_features)\n",
        "    return reduced_features, scaler, pca\n",
        "\n",
        "def apply_kmeans(features, n_clusters=3):\n",
        "    n_clusters = min(n_clusters, len(features))\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    cluster_labels = kmeans.fit_predict(features)\n",
        "    return cluster_labels, kmeans\n",
        "\n",
        "\n",
        "def train_isolation_forest(features):\n",
        "    iso_model = IsolationForest(random_state=42, contamination='auto')\n",
        "    iso_model.fit(features)\n",
        "    return iso_model\n",
        "\n",
        "def compute_cheating_score(model, features):\n",
        "    scores = model.decision_function(features)\n",
        "    anomalies = scores < 0\n",
        "    cheating_score = anomalies.sum() / len(scores)\n",
        "    return cheating_score\n",
        "\n",
        "def save_model(iso_model, scaler, pca_model, kmeans_model, filename=\"enhanced_model.pkl\"):\n",
        "    joblib.dump({\n",
        "        'isolation_forest': iso_model,\n",
        "        'scaler': scaler,\n",
        "        'pca': pca_model,\n",
        "        'kmeans': kmeans_model\n",
        "    }, filename)\n",
        "\n",
        "\n",
        "def load_model(filename=\"enhanced_model.pkl\"):\n",
        "    if os.path.exists(filename):\n",
        "        return joblib.load(filename)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def offline_training_pipeline(json_file):\n",
        "    data = load_json_data(json_file)\n",
        "    features, encoder = create_features(data)\n",
        "\n",
        "    if features.size == 0:\n",
        "        print(\"No activity detected. Saving empty model.\")\n",
        "        dummy_iso_model = IsolationForest().fit(np.zeros((1,3)))\n",
        "        save_model(dummy_iso_model, None, None, None)\n",
        "        return\n",
        "\n",
        "    reduced_features, scaler, pca_model = apply_pca(features)\n",
        "    cluster_labels, kmeans_model = apply_kmeans(reduced_features)\n",
        "\n",
        "    enhanced_features = np.hstack([reduced_features, cluster_labels.reshape(-1,1)])\n",
        "\n",
        "    iso_model = train_isolation_forest(enhanced_features)\n",
        "\n",
        "    save_model(iso_model, scaler, pca_model, kmeans_model)\n",
        "\n",
        "    cheating_score = compute_cheating_score(iso_model, enhanced_features)\n",
        "\n",
        "    print(f\"Offline training complete. Cheating Score: {cheating_score:.2f}\")\n",
        "\n",
        "\n",
        "def inference_pipeline(json_file):\n",
        "    model_package = load_model()\n",
        "\n",
        "    if model_package is None:\n",
        "        raise Exception(\"Model not found. Please run offline training first.\")\n",
        "\n",
        "    iso_model = model_package['isolation_forest']\n",
        "    scaler = model_package['scaler']\n",
        "    pca_model = model_package['pca']\n",
        "    kmeans_model = model_package['kmeans']\n",
        "\n",
        "    data = load_json_data(json_file)\n",
        "\n",
        "    features, encoder, descriptions = create_features(data)\n",
        "\n",
        "    if features.size == 0:\n",
        "        print(\"No activity detected. Cheating Score: 0.00\")\n",
        "        return 0.0\n",
        "\n",
        "    scaled_features = scaler.transform(features)\n",
        "    reduced_features = pca_model.transform(scaled_features)\n",
        "\n",
        "    cluster_labels = kmeans_model.predict(reduced_features)\n",
        "\n",
        "    enhanced_features = np.hstack([reduced_features, cluster_labels.reshape(-1,1)])\n",
        "\n",
        "    cheating_score = compute_cheating_score(iso_model, enhanced_features)\n",
        "\n",
        "    print(f\"Inference complete. Cheating Score: {cheating_score:.2f}\")\n",
        "\n",
        "    return cheating_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HcnA7hIKnfNl"
      },
      "outputs": [],
      "source": [
        "def create_features(data):\n",
        "    records = data.get(\"activityLog\", [])\n",
        "    if not records:\n",
        "        return np.array([]), None, []\n",
        "\n",
        "    feature_list, descriptions = [], []\n",
        "    for rec in records:\n",
        "        ts_val = convert_timestamp(rec.get(\"timeStampInVideo\", \"0:0:0\"))\n",
        "        count = rec.get(\"count\", 0)\n",
        "        description = rec.get(\"activityDescription\", \"\")\n",
        "        feature_list.append([ts_val, count])\n",
        "        descriptions.append(description)\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    desc_encoded = encoder.fit_transform(descriptions)\n",
        "    features = np.hstack([np.array(feature_list), desc_encoded.reshape(-1, 1)])\n",
        "    return features, encoder, descriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s_b5X_-6oSWK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_activity_data(json_file):\n",
        "    data = load_json_data(json_file)\n",
        "    records = data.get(\"activityLog\", [])\n",
        "\n",
        "    if not records:\n",
        "        print(\"No activity data to visualize\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    df['seconds'] = df['timeStampInVideo'].apply(convert_timestamp)\n",
        "\n",
        "    df = df.sort_values('seconds')\n",
        "\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(12, 15))\n",
        "\n",
        "    ax = axes[0]\n",
        "    sns.scatterplot(x='seconds', y='activityDescription', data=df,\n",
        "                    size='count', sizes=(20, 200), ax=ax)\n",
        "    ax.set_title('Activity Timeline')\n",
        "    ax.set_xlabel('Time (seconds)')\n",
        "    ax.set_ylabel('Activity Type')\n",
        "    ax = axes[1]\n",
        "    activity_counts = df['activityDescription'].value_counts()\n",
        "    activity_counts.plot(kind='barh', ax=ax)\n",
        "    ax.set_title('Activity Frequency')\n",
        "    ax.set_xlabel('Count')\n",
        "    ax = axes[2]\n",
        "    bin_size = 60\n",
        "    max_time = df['seconds'].max()\n",
        "    bins = list(range(0, max_time + bin_size, bin_size))\n",
        "    df['time_bin'] = pd.cut(df['seconds'], bins)\n",
        "\n",
        "    activity_density = df.groupby('time_bin').size()\n",
        "    activity_density.plot(kind='bar', ax=ax)\n",
        "    ax.set_title('Activity Density Over Time')\n",
        "    ax.set_xlabel('Time Range (seconds)')\n",
        "    ax.set_ylabel('Number of Activities')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{json_file.split('.')[0]}_visualization.png\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "\n",
        "    time_bins = pd.cut(df['seconds'], bins)\n",
        "    heatmap_data = pd.crosstab(time_bins, df['activityDescription'])\n",
        "\n",
        "    sns.heatmap(heatmap_data, cmap=\"YlOrRd\", linewidths=.5)\n",
        "    plt.title('Activity Patterns Over Time')\n",
        "    plt.xlabel('Activity Type')\n",
        "    plt.ylabel('Time Range (seconds)')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{json_file.split('.')[0]}_heatmap.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zwA0-4e6oiKw"
      },
      "outputs": [],
      "source": [
        "def predict():\n",
        "    json_file = \"candidate45.json\"\n",
        "    if not os.path.exists(json_file):\n",
        "        print(f\"[Warning] {json_file} not found. Skipping...\")\n",
        "        return\n",
        "    visualize_activity_data(json_file)\n",
        "    inference_pipeline(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D4mY_TIJOkwx",
        "outputId": "30954d96-4717-472c-af37-7f72b19cf29d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing data/candidate1.json...\n",
            "Processing data/candidate2.json...\n",
            "Processing data/candidate3.json...\n",
            "Processing data/candidate4.json...\n",
            "Processing data/candidate5.json...\n",
            "Processing data/candidate6.json...\n",
            "Processing data/candidate7.json...\n",
            "Processing data/candidate8.json...\n",
            "Processing data/candidate9.json...\n",
            "Processing data/candidate10.json...\n",
            "Processing data/candidate11.json...\n",
            "Processing data/candidate12.json...\n",
            "Processing data/candidate13.json...\n",
            "Processing data/candidate14.json...\n",
            "Processing data/candidate15.json...\n",
            "Processing data/candidate16.json...\n",
            "Processing data/candidate17.json...\n",
            "Processing data/candidate18.json...\n",
            "Processing data/candidate19.json...\n",
            "Processing data/candidate20.json...\n",
            "Processing data/candidate21.json...\n",
            "Processing data/candidate22.json...\n",
            "Processing data/candidate23.json...\n",
            "Processing data/candidate24.json...\n",
            "Processing data/candidate25.json...\n",
            "Processing data/candidate26.json...\n",
            "Processing data/candidate27.json...\n",
            "Processing data/candidate28.json...\n",
            "Processing data/candidate29.json...\n",
            "Processing data/candidate30.json...\n",
            "Processing data/candidate31.json...\n",
            "Processing data/candidate32.json...\n",
            "Processing data/candidate33.json...\n",
            "Processing data/candidate34.json...\n",
            "Processing data/candidate35.json...\n",
            "Processing data/candidate36.json...\n",
            "Processing data/candidate37.json...\n",
            "Processing data/candidate38.json...\n",
            "Processing data/candidate39.json...\n",
            "Processing data/candidate40.json...\n",
            "[[   2    1    8]\n",
            " [   9    2    8]\n",
            " [  13    1    8]\n",
            " ...\n",
            " [   9    1    2]\n",
            " [  10    1    0]\n",
            " [3600    1    1]]\n",
            "[Warning] candidate45.json not found. Skipping...\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    all_features = []\n",
        "\n",
        "\n",
        "    for i in range(1, 41):\n",
        "        json_file = f\"data/candidate{i}.json\"\n",
        "        if not os.path.exists(json_file):\n",
        "            print(f\"[Warning] {json_file} not found. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {json_file}...\")\n",
        "        data = load_json_data(json_file)\n",
        "        features, encoder, descriptions = create_features(data)\n",
        "\n",
        "        if features.size == 0:\n",
        "            print(f\"[Info] No activity detected in {json_file}, skipping feature aggregation.\")\n",
        "            continue\n",
        "\n",
        "        all_features.append(features)\n",
        "\n",
        "    if len(all_features) == 0:\n",
        "        raise Exception(\"No valid data found in any JSON files. Cannot train model.\")\n",
        "\n",
        "    combined_features = np.vstack(all_features)\n",
        "    reduced_features, scaler, pca_model = apply_pca(combined_features)\n",
        "    cluster_labels, kmeans_model = apply_kmeans(reduced_features)\n",
        "    enhanced_features = np.hstack([reduced_features, cluster_labels.reshape(-1, 1)])\n",
        "    iso_model = train_isolation_forest(enhanced_features)\n",
        "    save_model(iso_model, scaler, pca_model, kmeans_model)\n",
        "    predict()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
